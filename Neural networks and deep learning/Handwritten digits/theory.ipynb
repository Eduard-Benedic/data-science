{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons\n",
    "\n",
    "Just like a perceptron the sigmoid neurons has weights ($x1, x2, ... x_n$) and a $bias$. But, instead of outputing 0 or 1 the output is passed through the $sigmoid \\space function$. $\\sigma(w \\cdot x + b)$.\n",
    "\n",
    "$$ \\sigma(w \\cdot j + b) = \\frac{1}{1 + e^{-z}}, z = w \\cdot x + b $$\n",
    "$$ w \\cdot b = \\sum_{j} x_j \\cdot w_j + b $$\n",
    "\n",
    "The sigmoid function can be vizualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sigmoid = lambda z: 1 / (1 + np.e**-z)\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The shape of the sigmoid function is what's important. The exact shape doesn't really matter but it will simplify the calculations. If we make small changes to the weights and biases so that the sigmoid function is in that region of smoothness we can control the output. So changes in $\\Delta w$ or $\\Delta b$ will cause small changes in $\\Delta output$. Mathematically\n",
    "\n",
    "$$ \\Delta output \\approx \\sum_{j} \\frac{\\partial output}{\\partial w_j} \\cdot \\Delta w_j + \\frac{\\partial output}{\\partial b} \\cdot \\Delta b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ x - training \\space input$\n",
    " \n",
    "$ y = y(x) - desired \\space output $\n",
    "\n",
    "When we need is an algorithm that will adjust the weights and biases to produce an output close to the desired output. To quantify this we use a **cost function**(aka loss or objective).\n",
    "\n",
    "$$ C(w, b) = \\frac{1}{2n} \\sum_x || y(x) - a ||^2 $$\n",
    "$ w - all \\space weights $\n",
    "\n",
    "$ b - all \\space biases $\n",
    "\n",
    "$ a - output \\space when \\space x \\space input $\n",
    "\n",
    "This particular *cost function* is known as *the quadratic cost function* or *mean squared error* or just MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the cost function is finding the weights and biases that will reduce that cost function. Gradient descent is a common technique used to minize a function.\n",
    "\n",
    "Assuming C is a function that depends on v1, v2 we know from calculus:\n",
    "\n",
    "$$ \\Delta C \\approx \\frac{\\partial C }{\\partial v1} \\cdot \\Delta v1 + \\frac{\\partial C }{\\partial v2} \\cdot \\Delta v2 $$\n",
    "\n",
    "So we are going to find v1, v2 to make the $ \\Delta C $ negative.\n",
    "\n",
    "We can define $ \\Delta v $ as the vector of changes.\n",
    "\n",
    "$$ \\Delta v = (\\Delta v1, \\Delta v2)^T $$\n",
    "\n",
    "And define the partial derivatives with respect to each variable as the gradient. That's what is the gradient in fact.\n",
    "\n",
    "$$ \\nabla C = (\\frac{\\partial C}{\\partial v1}, \\frac{\\partial C}{\\partial v2})^T  $$\n",
    "\n",
    "With all the definitions we can express the equation for $ \\Delta C $ as a dot product.\n",
    "\n",
    "$$ \\Delta C \\approx \\nabla C \\cdot \\Delta v $$\n",
    "\n",
    "This equation is very important because it shows that if we choose $ \\Delta v $ to be $ \\eta \\cdot \\nabla C  $ ($\\eta$ known as *learning rate*) then we will be sure that the $ \\Delta C $ will alway decrease never increase.\n",
    "\n",
    "$$ \\Delta v = -\\eta \\cdot \\nabla C $$\n",
    "$$ \\Delta C \\approx - \\eta \\cdot \\nabla C = - \\eta || \\nabla C ||^2 $$\n",
    "\n",
    "because $ || \\nabla C ||^2  \\ge 0  $ within the boundaries of our approximation.\n",
    "\n",
    "THat is we will compute a value for $ \\Delta v $ and move the ball postition $ v $ by the amount.\n",
    "\n",
    "$$ v \\rightarrow v' = v - \\eta \\nabla C $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation algorithm\n",
    "\n",
    "Backgpropagation is an algorithm used by a neural network to train. At its core is an expression between each weight, bias and the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventions used for weights in the network\n",
    "\n",
    "$$ w_{jk}^{l} \\newline w - weight$$\n",
    "weight of the $j-th$ neuron in the $l$ layer and $k-th$ neuron in $l-1$ layer\n",
    "\n",
    "$$ b_j^l \\newline b - bias $$\n",
    "\n",
    "So we can succintly write.\n",
    "\n",
    "$$ a^l_k =  \\sigma(\\sum_k w_{jk}^l a_k^{l-1} + b_l^j) $$\n",
    "\n",
    "We can write this in a vectorized form.\n",
    "\n",
    "$$ a^l = \\sigma(w^la^{l-1} + b^l) $$\n",
    "\n",
    "The intermediate quantity inside the $\\sigma$ function its useful enough to be worth naming.\n",
    "\n",
    "$$ z^l \\equiv w^la^{l-1} + b^l$$\n",
    "$$ z^l - weighted \\space input$$\n",
    "\n",
    "Sometimes we can write the activation matrix of a layer in terms of the weigthed input like so:\n",
    "\n",
    "$$ a^l = \\sigma(z^l) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hadamard product ($\\bigodot$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$Hadamard \\space or \\space Schur \\space product$\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n",
    "\\bigodot\n",
    "\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1* 3 \\\\ 2 * 4 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 3 \\\\ 8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Backpropagatian is all about computing the partial derivatives of the cost function with respect with every single weight and bias. Mathematically we can express those as $ \\frac{\\partial C}{\\partial w^l_{jk}} $ and $ \\frac{\\partial C}{b_j^l} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute those we introduce an intermediat quantity called the error for the $j-th$ neuron in the $l-th$ layer\n",
    "\n",
    "$$\\delta^l_j = \\frac{\\partial C}{\\partial z_j^l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four equations for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An equation for the error in the output layer\n",
    "\n",
    "$$ \\delta_{j}^L = \\frac{\\partial C}{\\partial a_j^L} \\sigma'(z_j^L) $$\n",
    "\n",
    "The first term $ \\frac{\\partial C}{\\partial a_j^L} $ measures how much the cost is changing as a function of the $h-th$ output activation. The second term $\\sigma'(z_j^L) $ measures  how fast the activation function $ \\sigma $ is changing.\n",
    "\n",
    "The matrix based expression can be rewritten as:\n",
    "\n",
    "$$ \\delta^L = \\nabla_a C \\bigodot \\sigma'(z^L) $$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37229085c05107bf3fef04529a708b7ca0992a74d578c9c1a5b8eeb9cc4bc8cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
